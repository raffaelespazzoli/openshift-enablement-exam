# This is an example of a bring your own (byo) host inventory

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd
nfs
#{% if {{ env[GLUSTER] }} equals 'yes' %}
#glusterfs
#{% endif %}

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a
# password. If using ssh key based auth, then the key should be managed by an
# ssh agent.
ansible_ssh_user=scottes

# If ansible_ssh_user is not root, ansible_become must be set to true and the
# user must be configured for passwordless sudo
ansible_become=yes

# Debug level for all OpenShift components (Defaults to 2)
debug_level=2

# deployment type valid values are origin, online, atomic-enterprise, and openshift-enterprise
deployment_type=openshift-enterprise

# Specify the generic release of OpenShift to install. This is used mainly just during installation, after which we
# rely on the version running on the first master. Works best for containerized installs where we can usually
# use this to lookup the latest exact version of the container images, which is the tag actually used to configure
# the cluster. For RPM installations we just verify the version detected in your configured repos matches this
# release.
openshift_release=v3.6

# Docker Configuration
# Add additional, insecure, and blocked registries to global docker configuration
# For enterprise deployment types we ensure that registry.access.redhat.com is
# included if you do not include it
openshift_docker_insecure_registries=docker-registry.default.svc.cluster.local:5000, docker-registry.default.svc:5000
openshift_docker_options="--log-driver=json-file --log-opt max-size=50m --log-opt max-file=100 --storage-opt dm.basesize=15G"

# htpasswd auth
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
# Defining htpasswd users
#openshift_master_htpasswd_users={'admin': '$apr1$7aiANAYb$TOUYVUqnBqBlD5AQEIMYw1'} 
# or
#openshift_master_htpasswd_file=<path to local pre-generated htpasswd file>

# Allow all auth
#openshift_master_identity_providers=[{'name': 'allow_all', 'login': 'true', 'challenge': 'true', 'kind': 'AllowAllPasswordIdentityProvider'}]

# LDAP auth
#openshift_master_identity_providers=[{'name': 'my_ldap_provider', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['dn'], 'email': ['mail'], 'name': ['cn'], 'preferredUsername': ['uid']}, 'bindDN': '', 'bindPassword': '', 'ca': '', 'insecure': 'false', 'url': 'ldap://ldap.example.com:389/ou=users,dc=example,dc=com?uid'}]
# Configuring the ldap ca certificate
#openshift_master_ldap_ca=<ca text>
# or
#openshift_master_ldap_ca_file=<path to local ca file to use>

osm_default_node_selector='region=primary'


# GCE
openshift_cloudprovider_kind=gce

# Configure additional projects
#openshift_additional_projects={'my-project': {'default_node_selector': 'label=value'}}

# Enable cockpit
osm_use_cockpit=true
#
# Set cockpit plugins
osm_cockpit_plugins=['cockpit-kubernetes']

# Native high availability cluster method with optional load balancer.
# If no lb group is defined, the installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=mi.ocp.scottes.io
openshift_master_cluster_public_hostname=master.ocp.scottes.io

# default subdomain to use for exposed routes
openshift_master_default_subdomain=apps.ocp.scottes.io

# OpenShift Router Options
#
# An OpenShift router will be created during install if there are
# nodes present with labels matching the default router selector,
# "region=infra". Set openshift_node_labels per node as needed in
# order to label nodes.
#
openshift_hosted_router_selector='region=infra'
openshift_hosted_manage_router=true

# Openshift Registry Options
#
# An OpenShift registry will be created during install if there are
# nodes present with labels matching the default registry selector,
# "region=infra". Set openshift_node_labels per node as needed in
# order to label nodes.
#
openshift_hosted_registry_selector='region=infra'
openshift_hosted_manage_registry=true

# Metrics deployment
# See: https://docs.openshift.com/enterprise/latest/install_config/cluster_metrics.html
#
# By default metrics are not automatically deployed, set this to enable them
#openshift_hosted_metrics_deploy=true
#openshift_hosted_metrics_deployer_version=3.3.0
#openshift_hosted_metrics_storage_kind=nfs
#openshift_hosted_metrics_storage_volume_size=10Gi
#openshift_metrics_hawkular_replicas=2
#openshift_metrics_cassandra_replicas=3
#openshift_metrics_hawkular_nodeselector='{"region":"infra"}'
#openshift_metrics_cassandra_nodeselector='{"region":"infra"}'
#openshift_metrics_heapster_nodeselector='{"region":"infra"}'
#openshift_metrics_selector="region=infra"


# Logging deployment
#
# Currently logging deployment is disabled by default, enable it by setting this
#openshift_hosted_logging_deploy=false
openshift_hosted_logging_deploy=true
#openshift_hosted_logging_deployer_version=3.3.0
openshift_logging_image_version=v3.6
#openshift_logging_es_pvc_size=100Gi
openshift_logging_es_cluster_size=3
#openshift_logging_es_number_of_replicas=2
#openshift_logging_kibana_replica_count=2
openshift_logging_es_nodeselector='{"region":"infra"}'
openshift_logging_kibana_nodeselector='{"region":"infra"}'
openshift_logging_curator_nodeselector='{"region":"infra"}'

#openshift_hosted_logging_storage_host=host
openshift_hosted_logging_storage_kind=nfs
openshift_hosted_logging_storage_access_modes=['ReadWriteOnce']
openshift_hosted_logging_storage_nfs_directory=/exports
openshift_hosted_logging_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_logging_storage_volume_name=logging
openshift_hosted_logging_storage_volume_size=10Gi
openshift_hosted_logging_storage_labels={'storage': 'logging'}
openshift_logging_es_pvc_dynamic=false
openshift_logging_es_ops_pvc_dynamic=false
#openshift_logging_kibana_hostname=
#openshift_hosted_logging_hostname=

#openshift_hosted_logging_storage_kind=nfs
#openshift_hosted_logging_storage_access_modes=['ReadWriteOnce']
#openshift_hosted_logging_storage_host=ose-bastion.c.ocp-demo.internal
#openshift_hosted_logging_storage_nfs_directory=/NotBackedUp/nfs/ose36
#openshift_hosted_logging_storage_volume_name=logging
#openshift_hosted_logging_storage_volume_size=10Gi
#openshift_hosted_logging_storage_labels={'storage': 'logging'}

# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
#os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'


# Disable the OpenShift SDN plugin
# openshift_use_openshift_sdn=False

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
osm_cluster_network_cidr=10.1.0.0/16
#openshift_portal_net=172.30.0.0/16


# ExternalIPNetworkCIDRs controls what values are acceptable for the
# service external IP field. If empty, no externalIP may be set. It
# may contain a list of CIDRs which are checked for access. If a CIDR
# is prefixed with !, IPs in that CIDR will be rejected. Rejections
# will be applied first, then the IP checked against one of the
# allowed CIDRs. You should ensure this range does not overlap with
# your nodes, pods, or service CIDRs for security reasons.
#openshift_master_external_ip_network_cidrs=['0.0.0.0/0']

# Configure number of bits to allocate to each hostâ€™s subnet e.g. 8
# would mean a /24 network on the host.
#osm_host_subnet_length=8

# Configure dnsIP in the node config
#openshift_dns_ip=172.30.0.1

# Configure node kubelet arguments
openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['250'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80'], 'kube-reserved': ['cpu=100m,memory=300M'], 'system-reserved': ['cpu=100m,memory=100M']}

# Configure logrotate scripts
# See: https://github.com/nickhammond/ansible-logrotate
#logrotate_scripts=[{"name": "syslog", "path": "/var/log/cron\n/var/log/maillog\n/var/log/messages\n/var/log/secure\n/var/log/spooler\n", "options": ["daily", "rotate 7", "compress", "sharedscripts", "missingok"], "scripts": {"postrotate": "/bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true"}}]


#service broker settings

openshift_enable_service_catalog=false
openshift_template_service_broker_namespaces=['openshift']

#openshift_hosted_etcd_storage_kind=nfs
#openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
#openshift_hosted_etcd_storage_nfs_directory=/exports 
#openshift_hosted_etcd_storage_volume_name=etcd-vol2 
#openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
#openshift_hosted_etcd_storage_volume_size=1G
#openshift_hosted_etcd_storage_labels={'storage': 'etcd'}


#cloudforms settings
openshift_cfme_install_app=true

# checks
openshift_disable_check=docker_storage,memory_availability,disk_availability

#{% if {{ env[GLUSTER] }} equals = 'yes' %}
#gluster
#openshift_storage_glusterfs_namespace=glusterfs 
#openshift_storage_glusterfs_name=storage
#{% endif %}


# host group for masters
[masters]
master[1:3]

[etcd]
master[1:3]

[nfs]
ose-bastion

# NOTE: Currently we require that masters be part of the SDN which requires that they also be nodes
# However, in order to ensure that your masters are not burdened with running pods you should
# make them unschedulable by adding openshift_schedulable=False any node that's also a master.
[nodes]
master[1:3] openshift_schedulable=false openshift_node_labels="{'region': 'primary'}" 
node[1:3] openshift_node_labels="{'region': 'primary'}" 
infranode[1:3] openshift_node_labels="{'region': 'infra'}"


#{% if {{ env[GLUSTER] }} equals 'yes' %}
#[glusterfs] 
#node[1:3] glusterfs_ip="{{ ansible_default_ipv4.address }}" glusterfs_devices='[ "/dev/sdc" ]'
#{% endif %}

